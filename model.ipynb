{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/pycodesuggest/github-scraper')\n",
    "sys.path.append('/pycodesuggest/lm')\n",
    "\n",
    "import astwalker\n",
    "import evaluation\n",
    "import prebatcher\n",
    "import pyreader\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import datetime\n",
    "\n",
    "from Trainer import Trainer\n",
    "from batcher import QueuedSequenceBatcher, PreBatched\n",
    "from hooks import SpeedHook, LossHook, SaveModelHook\n",
    "from hooks import PerplexityHook\n",
    "\n",
    "from utils import save_model, load_model, load_variables\n",
    "from utils import get_file_list, copy_temp_files, create_model\n",
    "\n",
    "flags = tf.flags\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "flags.DEFINE_string(\"data_path\", \"/data_normalised\", \"data path\")\n",
    "flags.DEFINE_boolean(\"train\", False, \"train the model\")\n",
    "flags.DEFINE_boolean(\"test\", False, \"test the model\")\n",
    "flags.DEFINE_boolean(\"preprocess\", False, \"Proprocess data\")\n",
    "flags.DEFINE_boolean(\"prebatch\", False, \"Pre-batch and split the data\")\n",
    "flags.DEFINE_boolean(\"vocab\", False, \"Generate vocab\")\n",
    "flags.DEFINE_string(\"list_file\", \"train_files.txt\", \"Name of the list file found in data_path\")\n",
    "flags.DEFINE_string(\"vocab_file\", \"mapping.map\", \"Name of the vocab file in data_path\")\n",
    "flags.DEFINE_string(\"output_file\", \"/data_normalised/out/all_train_data.dat\", \"Name of the output file\")\n",
    "flags.DEFINE_boolean(\"debug\", False, \"Use debug config\")\n",
    "flags.DEFINE_integer(\"seq_length\", 100, \"Sequence length\")\n",
    "flags.DEFINE_integer(\"batch_size\", 100, \"Batch size\")\n",
    "flags.DEFINE_integer(\"num_partitions\", 10, \"Data partitions\")\n",
    "flags.DEFINE_boolean(\"use_prebatched\", True, \"Use prebatched data\")\n",
    "flags.DEFINE_boolean(\"copy_temp\", False, \"Copy data to local temp directory\")\n",
    "flags.DEFINE_integer(\"oov_threshold\", 0, \"Out of vocabulary threshold\")\n",
    "flags.DEFINE_integer(\"epochs\", 50, \"Number of epochs to run\")\n",
    "flags.DEFINE_string(\"attention\", \"identifiers\" or \"\", \"Use the attention model\")\n",
    "flags.DEFINE_string(\"attention_variant\", \"input\", \"Variation of attention model to use. Possible values are: \"\n",
    "                                               \"input, output\")\n",
    "flags.DEFINE_float(\"init_scale\", 0.1, \"Initialisation scale\")\n",
    "flags.DEFINE_integer(\"max_grad_norm\", 5, \"Maximum norm for gradients\")\n",
    "flags.DEFINE_integer(\"num_layers\", 1, \"Number of LSTM layers\")\n",
    "flags.DEFINE_integer(\"hidden_size\", 200, \"Size of hidden state\")\n",
    "flags.DEFINE_float(\"keep_prob\", 0.9, \"1 - probability of dropout of input\")\n",
    "flags.DEFINE_integer(\"num_samples\", 100, \"Number of samples for sampled softmax\")\n",
    "flags.DEFINE_integer(\"status_iterations\", 1000, \"Number of iterations before status messages are displayed\")\n",
    "flags.DEFINE_integer(\"max_attention\", 10, \"Maximum size of attention matrix\")\n",
    "flags.DEFINE_float(\"learning_rate\", 1.0, \"Gradient Descent Learning Rate\")\n",
    "flags.DEFINE_float(\"lr_decay\", 0.9, \"Learning rate decay factor\")\n",
    "flags.DEFINE_string(\"model_path\", \"./out/model/latest\" and None, \"Model parameters to load. If train=True, \"\n",
    "                                        \"will continue training from these parameters. If test=True,\"\n",
    "                                        \"will test using these model parameters\")\n",
    "flags.DEFINE_string(\"lambda_type\", \"state\", \"Method to calculate lambda, possible values are fixed, state, att, input.\"\n",
    "                                            \"state, att and input can be combined using + \"\n",
    "                                            \"eg state+att for state and attention\")\n",
    "flags.DEFINE_string(\"save_path\", \"./out/model\", \"Path to save the final model\")\n",
    "flags.DEFINE_string(\"checkpoint_path\", \"./out/checkpoint\", \"Path to save intermediate checkpoints, every 5 epochs\")\n",
    "flags.DEFINE_string(\"events_path\", \"./out/save\", \"Path to save summary events\")\n",
    "flags.DEFINE_string(\"data_pattern\", \"all_{-type-}_data.dat\", \"Pattern for data files, use {-type-} as a placeholder\"\n",
    "                                                             \"for train/valid/test and leave out the part extension\")\n",
    "flags.DEFINE_boolean(\"vocab_incl_ids\", False, \"Include normalised identifiers in the vocab\")\n",
    "flags.DEFINE_string(\"embedding_path\", None, \"Path to load embeddings from\")\n",
    "flags.DEFINE_boolean(\"embedding_trainable\", True, \"Flag indicating whether the embeddings are trainable or not.\"\n",
    "                                                  \"Useful if embeddings are provided with embedding_path which should\"\n",
    "                                                  \"not be updated during training\")\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
